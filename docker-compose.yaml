services:
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ./docker/ollama_files:/root/.ollama
      - ./ollama_entrypoint.sh:/ollama_entrypoint.sh
    environment:
      - LLM_MODEL_VERSION=${LLM_MODEL_VERSION} 
    entrypoint: ["/bin/bash", "/ollama_entrypoint.sh"] 
  
  app:
    build: .
    container_name: wav-chat
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ./docker/files:/usr/src/app/files
    stdin_open: true
    tty: true